{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def step_function (x):\n",
    "    return 1 if x > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptron training function\n",
    "class Perceptron:\n",
    "    def __init__(self, learning_rate=0.1, epochs=10):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "# learning_rate: Controls the step size of weight updates.\n",
    "# epochs: Number of times the entire dataset is processed during training.\n",
    "# weights: Stores the weights of the perceptron, initialized later.\n",
    "# bias: An additional parameter to adjust \n",
    "# decision boundaries, also initialized later.\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.weights = np.zeros(num_features)\n",
    "        self.bias = 0\n",
    "## Initializes weights to a zero vector and bias to 0.\n",
    "## Iterates through the data for the specified number of epochs\n",
    "\n",
    "        # Training\n",
    "        for _ in range(self.epochs):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                linear_output = np.dot(x_i, self.weights) + self.bias\n",
    "                y_pred = step_function(linear_output)\n",
    "\n",
    "## Computes the weighted sum (linear_output) and \n",
    "## applies the step function (y_pred)\n",
    "\n",
    "                # Update weights and bias\n",
    "                update = self.learning_rate * (y[idx] - y_pred)\n",
    "                self.weights += update * x_i\n",
    "                self.bias += update\n",
    "\n",
    "### If thereâ€™s a misclassification (y[idx] - y_pred), \n",
    "### updates the weights and bias based on the error and learning rate.\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_output = np.dot(X, self.weights) + self.bias\n",
    "        return np.array([step_function(x) for x in linear_output])\n",
    "    \n",
    "### Computes the output for given inputs and \n",
    "### applies the step function to return predictions.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example dataset (AND logic)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m], [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m], [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m]])\n\u001b[0;32m      3\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m])  \u001b[38;5;66;03m# AND logic\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m### X represents input pairs.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m### y represents the AND operation output: \u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m### only 1 when both inputs are 1.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Train perceptron\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Example dataset (AND logic)\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 0, 0, 1])  # AND logic\n",
    "\n",
    "### X represents input pairs.\n",
    "### y represents the AND operation output: \n",
    "### only 1 when both inputs are 1.\n",
    "\n",
    "# Train perceptron\n",
    "perceptron = Perceptron(learning_rate=0.1, epochs=10)\n",
    "perceptron.fit(X, y)\n",
    "\n",
    "### Creates a perceptron instance and trains it on the AND dataset.\n",
    "\n",
    "# Test predictions\n",
    "print(\"Predictions:\", perceptron.predict(X))\n",
    "### Evaluates the perceptron on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_or = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_or = np.array([0, 1, 1, 1])  # OR logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron_or = Perceptron(learning_rate=0.1, epochs=10)\n",
    "perceptron_or.fit(X_or, y_or)\n",
    "print(\"OR Predictions:\", perceptron_or.predict(X_or))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AND logic dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 0, 0, 1])  # AND logic\n",
    "\n",
    "# Perceptron training and testing\n",
    "perceptron = Perceptron(learning_rate=0.5, epochs=100)  # Adjusted learning rate and epochs\n",
    "perceptron.fit(X, y)\n",
    "\n",
    "# Test predictions\n",
    "predictions = perceptron.predict(X)\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# AND logic dataset\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# AND logic dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y = np.array([[0], [0], [0], [1]], dtype=np.float32)  # AND logic\n",
    "\n",
    "# Define the model\n",
    "class ANDModel(tf.Module):\n",
    "    def __init__(self):\n",
    "        self.weights = tf.Variable(tf.random.normal([2, 1]))\n",
    "        self.bias = tf.Variable(tf.random.normal([1]))\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        linear_output = tf.matmul(inputs, self.weights) + self.bias\n",
    "        return tf.sigmoid(linear_output)\n",
    "\n",
    "# Training function\n",
    "def train(model, inputs, outputs, learning_rate=0.1, epochs=1000):\n",
    "    optimizer = tf.optimizers.SGD(learning_rate)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            linear_output = tf.matmul(inputs, model.weights) + model.bias\n",
    "            predictions = tf.sigmoid(linear_output)\n",
    "            loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(outputs, predictions))\n",
    "\n",
    "        gradients = tape.gradient(loss, [model.weights, model.bias])\n",
    "        optimizer.apply_gradients(zip(gradients, [model.weights, model.bias]))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "model = ANDModel()\n",
    "trained_model = train(model, X, y)\n",
    "\n",
    "# Test predictions\n",
    "predictions = trained_model(X).numpy()\n",
    "predictions_binary = (predictions > 0.5).astype(int)\n",
    "print(\"Predictions:\", predictions_binary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
